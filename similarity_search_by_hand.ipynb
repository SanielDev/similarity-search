{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd2c3f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers==4.1.0) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers==4.1.0 | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfc1a6f",
   "metadata": {},
   "source": [
    "- numpy for mathematical operations.\n",
    "- scipy for additional mathematical operations not found in numpy.\n",
    "- torch for vector operations, although this library is also typically used for deep learning.\n",
    "- sentence_transformers for obtaining vector embeddings from text data using pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f693f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanielbhattarai/Desktop/IBM Certification/Vector Databases for RAG/Module-1/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d1717d",
   "metadata": {},
   "source": [
    "## Obtain Vector Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6063804e",
   "metadata": {},
   "source": [
    "To calculate distance and similarity metrics, we first need to generate vector embeddings for some text documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36e114c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example documents\n",
    "documents = [\n",
    "    'Bugs introduced by the intern had to be squashed by the lead developer.',\n",
    "    'Bugs found by the quality assurance engineer were difficult to debug.',\n",
    "    'Bugs are common throughout the warm summer months, according to the entomologist.',\n",
    "    'Bugs, in particular spiders, are extensively studied by arachnologists.'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5560c1a",
   "metadata": {},
   "source": [
    "As shown above, there are four example documents, each consisting of a single sentence. These sentences are intentionally designed to be challenging for semantic similarity search.\n",
    "\n",
    "All four sentences begin with the word \"Bugs,\" but they refer to different meanings of the word depending on context. The first two sentences relate to software bugs in programming, while the last two refer to physical bugs, such as insects or spiders.\n",
    "\n",
    "The key to distinguishing between these meanings lies in the context, particularly the type of professional mentioned in each sentence. For example, if we replaced the word \"arachnologists\" (scientists who study spiders and other arthropods) in the last sentence with \"lead developers,\" the sentence would instead refer to programming bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e93e3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2a3355",
   "metadata": {},
   "source": [
    "The code above creates an instance of the SentenceTransformer class from the sentence_transformers library, which is commonly used to generate vector embeddings from pre-trained models.\n",
    "\n",
    "In this example, we’re using the paraphrase-MiniLM-L6-v2 model. It was trained on pairs of paraphrased sentences, with the goal of generating similar embeddings for sentences that express the same meaning. While the model was originally designed for paraphrase identification, it also performs well on general semantic similarity tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "300f4b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "embeddings = model.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f73e5fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 384)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4f85dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.22804335, -0.246477  , -0.00319243, ...,  0.4552812 ,\n",
       "         0.6341975 ,  0.53750485],\n",
       "       [-0.35791564, -0.32084018,  0.15963301, ..., -0.07050626,\n",
       "         0.9275025 ,  0.34377253],\n",
       "       [ 0.20302981, -0.2689862 ,  0.1628514 , ..., -0.19650973,\n",
       "        -0.03379842,  0.5956156 ],\n",
       "       [-0.04264269, -0.45721576, -0.09526495, ..., -0.5803074 ,\n",
       "         0.17248403,  0.09127846]], shape=(4, 384), dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe1e4b8",
   "metadata": {},
   "source": [
    "As shown above, the four text documents have been converted into a NumPy array with shape (4, 384). Each row in the array represents the embedding of one document, and the number 384 is the dimensionality of the embeddings produced by the paraphrase-MiniLM-L6-v2 model. In other words, each document has been transformed into a numerical vector containing 384 values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60816a7",
   "metadata": {},
   "source": [
    "## L2 (Euclidean) Distance\n",
    "The L2 (Euclidean) distance between 2 vectors $a$ and $b$ can be calculated using <br>\n",
    "$$ \\text{L2}(a,b) = \\ \\sqrt{\\sum_{i=1}^n (a_i - b_i)^2} $$\n",
    "\n",
    "Let's try to implement the L2 distance manually before looking at off-the-shelf solutions available from third party libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00612ec0",
   "metadata": {},
   "source": [
    "### Manual implementation of L2 distance calculation\n",
    "The function below implements the L2 distance formula. It first calculates the sum of the squared differences between corresponding elements, then returns the square root of that sum:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37b5267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_fn(vector1, vector2):\n",
    "    squared_sum = sum((x - y) ** 2 for x, y in zip(vector1, vector2))\n",
    "    return math.sqrt(squared_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15b640f",
   "metadata": {},
   "source": [
    "Note that euclidean_distance_fn computes the distance between two individual vectors, not across an entire array. To see how it works, let's calculate the distance between the first vector (index 0) and the second vector (index 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5faad605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.96179017134276"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distance_fn(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bc5ad61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.96179017134276"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distance_fn(embeddings[1], embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae25753",
   "metadata": {},
   "source": [
    "In order to calculate all of the distances between all of the vectors in the embeddings array, we can use nested loops. In the following, i and j are the indices of the vectors in the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "412bff8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 5.96179017, 7.33939859, 7.15578223],\n",
       "       [5.96179017, 0.        , 7.76861699, 7.39359074],\n",
       "       [7.33939859, 7.76861699, 0.        , 5.919928  ],\n",
       "       [7.15578223, 7.39359074, 5.919928  , 0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_dist_manual = np.zeros([4,4])\n",
    "for i in range(embeddings.shape[0]):\n",
    "    for j in range(embeddings.shape[0]):\n",
    "        l2_dist_manual[i,j] = euclidean_distance_fn(embeddings[i], embeddings[j])\n",
    "\n",
    "l2_dist_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a276080",
   "metadata": {},
   "source": [
    "`l2_dist_manual` is a 4×4 array where each element represents the L2 distance between two vectors: the vector at the row index and the vector at the column index. For example, the distance between the first vector (index 0) and the second vector (index 1) is located at position `[0, 1]` in the array:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c5ae2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(5.96179017134276)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_dist_manual[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d602e639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(5.96179017134276)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_dist_manual[1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393a970d",
   "metadata": {},
   "source": [
    "#### Exercise 1 - Make the manual calculation more efficient\n",
    "\n",
    "The code used to populate the `l2_dist_manual array` is not very efficient. First, it redundantly calculates the distance between a vector and itself, even though the L2 distance in such cases is always zero. Second, the array is symmetric—meaning the distance between vectors at indices $i$ and $j$ is the same as between $j$ and $i$. Therefore, each distance only needs to be computed once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28cd0a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 5.96179017, 7.33939859, 7.15578223],\n",
       "       [5.96179017, 0.        , 7.76861699, 7.39359074],\n",
       "       [7.33939859, 7.76861699, 0.        , 5.919928  ],\n",
       "       [7.15578223, 7.39359074, 5.919928  , 0.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_dist_manual_improved = np.zeros([4,4])\n",
    "for i in range(embeddings.shape[0]):\n",
    "    for j in range(embeddings.shape[0]):\n",
    "        if i < j: \n",
    "            l2_dist_manual_improved[i,j] = euclidean_distance_fn(embeddings[i], embeddings[j])\n",
    "        else:\n",
    "            l2_dist_manual_improved[i,j] = l2_dist_manual_improved[j,i]\n",
    "\n",
    "l2_dist_manual_improved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43956ea",
   "metadata": {},
   "source": [
    "### Calculate L2 distance using `scipy`\n",
    "Instead of writing your own function, you can use a variety of different functions provided by external libraries to calculate the L2 distance. The example below uses a function from `scipy`: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ae0514e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 5.96179042, 7.33939999, 7.15578144],\n",
       "       [5.96179042, 0.        , 7.76861693, 7.39359088],\n",
       "       [7.33939999, 7.76861693, 0.        , 5.91992784],\n",
       "       [7.15578144, 7.39359088, 5.91992784, 0.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_dist_scipy = scipy.spatial.distance.cdist(embeddings, embeddings, 'euclidean')\n",
    "l2_dist_scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2734899a",
   "metadata": {},
   "source": [
    "The following verifies that `l2_dist_manual` and `l2_dist_scipy` are identical (after accounting for rounding errors) by using the `allclose()` function from `numpy`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e51f5ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(l2_dist_manual, l2_dist_scipy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d27854",
   "metadata": {},
   "source": [
    "### Interpret the L2 Distance Results\n",
    "\n",
    "An analysis of `l2_dist_scipy` shows that, in this case, the L2 distance metric performed well for similarity search. For example, the first vector—corresponding to the sentence `Bugs introduced by the intern had to be squashed by the lead developer.`—had the smallest distance to the second vector, which represents the sentence `Bugs found by the quality assurance engineer were difficult to debug.` This result aligns with expectations, as both sentences refer to programming bugs.\n",
    "\n",
    "Similarly, the third and fourth sentences—both related to physical bugs rather than programming—were closest to each other in terms of distance, which again matches our intuition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac9cd4b",
   "metadata": {},
   "source": [
    "## Dot Product Similarity and Distance\n",
    "The dot product between vectors $a$ and $b$ is calculated using  <br>\n",
    "$$ a \\cdot b = \\sum_{i=1}^{n} a_i b_i $$\n",
    "\n",
    "Let's define a custom function for calculating the dot product between two vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5dc862",
   "metadata": {},
   "source": [
    "### Manual implementation of dot product calculation\n",
    "The function below implements the dot product formula:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f1ba3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_fn(vector1, vector2):\n",
    "    return sum(x * y for x, y in zip(vector1, vector2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4467f7",
   "metadata": {},
   "source": [
    "Let's use the dot product function to calculate the dot product between the first and second vectors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39114431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(18.535397)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product_fn(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4eb44e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33.74440384, 18.53539658,  8.56981182,  7.83093166],\n",
       "       [18.53539658, 38.86933899,  7.8899622 ,  8.6634016 ],\n",
       "       [ 8.56981182,  7.8899622 , 37.26202011, 17.66955757],\n",
       "       [ 7.83093166,  8.6634016 , 17.66955757, 33.12266159]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product_manual = np.empty([4,4])\n",
    "for i in range(embeddings.shape[0]):\n",
    "    for j in range(embeddings.shape[0]):\n",
    "        dot_product_manual[i,j] = dot_product_fn(embeddings[i], embeddings[j])\n",
    "\n",
    "dot_product_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c716e44",
   "metadata": {},
   "source": [
    "First, observe that the diagonal of the dot_product_manual matrix contains non-zero values. This is expected, as the dot product of a vector with itself is not zero. However, since these self-similarity scores aren't particularly informative, we can safely ignore them.\n",
    "\n",
    "Excluding the diagonal, the matrix is symmetric—the upper triangle mirrors the lower triangle—as was the case with the L2 distance.\n",
    "\n",
    "It's also important to remember that the dot product measures similarity, not distance. Higher values indicate greater similarity between vectors.\n",
    "\n",
    "In this case, the dot product similarity performed well: the first two sentences are most similar to each other, and the last two sentences are most similar to each other, which aligns with our expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee460b0",
   "metadata": {},
   "source": [
    "### Calculate the dot product using matrix multiplication\n",
    "We can compute the dot product efficiently using the [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication) operator `@`. To do this, we multiply the `embeddings` matrix by its transpose. Since `embeddings` has a shape of 4×384, its transpose will be 384×4. Multiplying these gives us a 4×4 matrix, which is the desired result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b57afdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33.74442  , 18.5354   ,  8.569813 ,  7.8309317],\n",
       "       [18.5354   , 38.869335 ,  7.8899646,  8.663406 ],\n",
       "       [ 8.569813 ,  7.8899646, 37.262005 , 17.669558 ],\n",
       "       [ 7.8309317,  8.663406 , 17.669558 , 33.122658 ]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix multiplication operator\n",
    "dot_product_operator = embeddings @ embeddings.T\n",
    "dot_product_operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f4797f",
   "metadata": {},
   "source": [
    "We can verify that the matrix multiplication operator returns the same result as our custom function after accounting for rounding:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a38163fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(dot_product_manual, dot_product_operator, atol=1e-05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f6b266",
   "metadata": {},
   "source": [
    "Equivalently, if both of the matrices we want to multiply are two-dimensional, we can use the `matmul()` function from `numpy` instead:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac5d44fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33.74442  , 18.5354   ,  8.569813 ,  7.8309317],\n",
       "       [18.5354   , 38.869335 ,  7.8899646,  8.663406 ],\n",
       "       [ 8.569813 ,  7.8899646, 37.262005 , 17.669558 ],\n",
       "       [ 7.8309317,  8.663406 , 17.669558 , 33.122658 ]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Equivalent to `np.matmul()` if both arrays are 2-D:\n",
    "np.matmul(embeddings,embeddings.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e4da4",
   "metadata": {},
   "source": [
    "Finally, `numpy` also provides a `dot()` function, which provides an identical result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d317a29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33.74442  , 18.5354   ,  8.569813 ,  7.8309317],\n",
       "       [18.5354   , 38.869335 ,  7.8899646,  8.663406 ],\n",
       "       [ 8.569813 ,  7.8899646, 37.262005 , 17.669558 ],\n",
       "       [ 7.8309317,  8.663406 , 17.669558 , 33.122658 ]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `np.dot` returns an identical result, but `np.matmul` is recommended if both arrays are 2-D:\n",
    "np.dot(embeddings,embeddings.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8298b20e",
   "metadata": {},
   "source": [
    "### Calculate dot product distance\n",
    "The dot product between two vectors provides a similarity score. If, on the other hand, we would like a distance, we can simply take the negative of the dot product:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d24ac15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-33.74440384, -18.53539658,  -8.56981182,  -7.83093166],\n",
       "       [-18.53539658, -38.86933899,  -7.8899622 ,  -8.6634016 ],\n",
       "       [ -8.56981182,  -7.8899622 , -37.26202011, -17.66955757],\n",
       "       [ -7.83093166,  -8.6634016 , -17.66955757, -33.12266159]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product_distance = -dot_product_manual\n",
    "dot_product_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1162d0",
   "metadata": {},
   "source": [
    "Although it might seem unusual for all the distances to be negative, the essential property of a distance metric is still preserved: smaller values indicate lower distance and thus greater similarity. So even with negative values, the relative comparisons remain valid—lower values still correspond to shorter distances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cd415e",
   "metadata": {},
   "source": [
    "## Cosine Similarity and Distance\n",
    "The cosine similarity between vectors $a$ and $b$ is calculated using <br>\n",
    "$$ \\text{cossim}(a, b) = \\frac{a \\cdot b}{||a|| \\ ||b||} =  \\frac{a}{||a||} \\cdot \\frac{b}{||b||} $$\n",
    "\n",
    "where $||a|| = \\sqrt{\\sum_{k=1}^n a_k^2}$ is the L2 norm, or the magnitude, of vector $a$, and $\\cdot$ is the dot product.\n",
    "\n",
    "Also note that $\\frac{a}{||a||}$ represents a normalized vector. This means it has the same direction as vector $a$ but a magnitude (or length) of 1. Thus, cosine similarity can be calculated by taking the dot product of two normalized vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f926dc",
   "metadata": {},
   "source": [
    "### Manual implementation of cosine similarity calculation\n",
    "Since we’ve already covered how to compute the dot product, our strategy for manually calculating cosine similarity will focus on normalizing the vectors. This is because cosine similarity is simply the dot product of two normalized vectors, as was shown after the last equals sign in the cosine similarity calculation formula.\n",
    "\n",
    "However, in order to normalize vectors, we must first compute their L2 norms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af922ff",
   "metadata": {},
   "source": [
    "#### Calculate the L2 norm\n",
    "The following calculates the L2 norms for all the vectors in the `embeddings` array. The calculation simply squares each vector component, sums across columns (note the `axis=1` parameter in the `sum`), and takes a square root:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4cbc137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.8089943, 6.2345276, 6.1042614, 5.7552285], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2 norms\n",
    "l2_norms = np.sqrt(np.sum(embeddings**2, axis=1))\n",
    "l2_norms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195ee634",
   "metadata": {},
   "source": [
    "Note that the result is a vector with 4 numbers, each of which corresponds to the L2 norm, or the magnitude, of each vector. In order to normalize the vectors to a length of one, we should divide each vector's components by the norm. However, in order to do that efficiently, let's reshape the `l2_norms` vector into a 4x1 array:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5522a54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.8089943],\n",
       "       [6.2345276],\n",
       "       [6.1042614],\n",
       "       [5.7552285]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2 norms reshaped\n",
    "l2_norms_reshaped = l2_norms.reshape(-1,1)\n",
    "l2_norms_reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fcba64",
   "metadata": {},
   "source": [
    "#### Normalize embedding vectors\n",
    "The following code calculates normalized embedding vectors by dividing every component in the vector by the vector's L2 norm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cff27162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03925694, -0.04243023, -0.00054957, ...,  0.07837522,\n",
       "         0.10917509,  0.09252976],\n",
       "       [-0.05740862, -0.05146183,  0.02560467, ..., -0.011309  ,\n",
       "         0.1487687 ,  0.05514011],\n",
       "       [ 0.03326034, -0.04406532,  0.02667831, ..., -0.03219222,\n",
       "        -0.00553686,  0.09757374],\n",
       "       [-0.00740938, -0.07944354, -0.01655276, ..., -0.10083134,\n",
       "         0.02996997,  0.01586009]], shape=(4, 384), dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_embeddings_manual = embeddings/l2_norms_reshaped\n",
    "normalized_embeddings_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2bfc658f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         1.         0.99999994 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Verify normalization\n",
    "norms_after_normalization = np.sqrt(np.sum(normalized_embeddings_manual**2, axis=1))\n",
    "print(norms_after_normalization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6348c6",
   "metadata": {},
   "source": [
    "#### Normalize embeddings using PyTorch\n",
    "You can normalize embeddings in PyTorch using `torch.nn.functional.normalize()`. If your data is in a NumPy array, first convert it to a PyTorch tensor using `torch.from_numpy()`. After normalization, convert the tensor back to a NumPy array using the `numpy()` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b464c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03925694, -0.04243023, -0.00054957, ...,  0.07837522,\n",
       "         0.10917509,  0.09252976],\n",
       "       [-0.05740862, -0.05146183,  0.02560467, ..., -0.011309  ,\n",
       "         0.1487687 ,  0.05514011],\n",
       "       [ 0.03326034, -0.04406532,  0.02667831, ..., -0.03219222,\n",
       "        -0.00553686,  0.09757374],\n",
       "       [-0.00740938, -0.07944354, -0.01655276, ..., -0.10083134,\n",
       "         0.02996997,  0.01586009]], shape=(4, 384), dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_embeddings_torch = torch.nn.functional.normalize(\n",
    "    torch.from_numpy(embeddings)\n",
    ").numpy()\n",
    "normalized_embeddings_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fa59b1",
   "metadata": {},
   "source": [
    "We can verify that the normalized embeddings we calculated manually and the normalized embeddings calculated using `torch` are indeed identical using numpy's `allclose()` function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6430e12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(normalized_embeddings_manual, normalized_embeddings_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate cosine similarity manually\n",
    "To calculate cosine similarity between two normalized embedding vectors, we simply take their dot product. To do this, we can leverage the dot product function we defined before. For instance, the following calculates the cosine similarity between the vector embeddings of the first and second sentence:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b94668a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.5117967)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product_fn(normalized_embeddings_manual[0], normalized_embeddings_manual[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340208c3",
   "metadata": {},
   "source": [
    "Likewise, to calculate the cosine similarities between all normalized vectors, we can use a nested loop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55875953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999982, 0.51179671, 0.24167807, 0.23423398],\n",
       "       [0.51179671, 1.        , 0.20731872, 0.2414474 ],\n",
       "       [0.24167807, 0.20731872, 1.00000072, 0.50295585],\n",
       "       [0.23423398, 0.2414474 , 0.50295585, 1.00000024]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_manual = np.empty([4,4])\n",
    "for i in range(normalized_embeddings_manual.shape[0]):\n",
    "    for j in range(normalized_embeddings_manual.shape[0]):\n",
    "        cosine_similarity_manual[i,j] = dot_product_fn(\n",
    "            normalized_embeddings_manual[i], \n",
    "            normalized_embeddings_manual[j]\n",
    "        )\n",
    "\n",
    "cosine_similarity_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75500cca",
   "metadata": {},
   "source": [
    "### Calculate cosine similarity using matrix multiplication\n",
    "Just like with the dot product, we can compute cosine similarity using matrix algebra. By multiplying the matrix of normalized embeddings with its transpose using the matrix multiplication operator, we obtain the cosine similarity matrix. This works because, once vectors are normalized, cosine similarity can be calculated by simply taking the dot product:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc0b75fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999994, 0.5117967 , 0.24167809, 0.234234  ],\n",
       "       [0.5117967 , 1.0000001 , 0.20731868, 0.24144733],\n",
       "       [0.24167809, 0.20731868, 1.        , 0.50295603],\n",
       "       [0.234234  , 0.24144733, 0.50295603, 1.0000001 ]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_operator = normalized_embeddings_manual @ normalized_embeddings_manual.T\n",
    "cosine_similarity_operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aec8597",
   "metadata": {},
   "source": [
    "We can verify that the matrix algebra solution is the same as the one found using the nested loop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5802306c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(cosine_similarity_manual, cosine_similarity_operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0737bc7",
   "metadata": {},
   "source": [
    "### Calculate cosine distance\n",
    "The cosine distance between vectors $a$ and $b$ is simply 1 minus the cosine similarity between $a$ and $b$: <br>\n",
    "$$ 1 - cossim(a,b) $$\n",
    "\n",
    "Using numpy, this can be calculated as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd1adfd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.78813934e-07,  4.88203287e-01,  7.58321926e-01,\n",
       "         7.65766025e-01],\n",
       "       [ 4.88203287e-01,  0.00000000e+00,  7.92681277e-01,\n",
       "         7.58552596e-01],\n",
       "       [ 7.58321926e-01,  7.92681277e-01, -7.15255737e-07,\n",
       "         4.97044146e-01],\n",
       "       [ 7.65766025e-01,  7.58552596e-01,  4.97044146e-01,\n",
       "        -2.38418579e-07]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - cosine_similarity_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d7bce",
   "metadata": {},
   "source": [
    "## Exercise 3 - Similarity Search Using a Query\n",
    "In the above examples, we calculated similarity between 4 documents:\n",
    "\n",
    "```python\n",
    "documents = [\n",
    "    'Bugs introduced by the intern had to be squashed by the lead developer.',\n",
    "    'Bugs found by the quality assurance engineer were difficult to debug.',\n",
    "    'Bugs are common throughout the warm summer months, according to the entomologist.',\n",
    "    'Bugs, in particular spiders, are extensively studied by arachnologists.'\n",
    "]\n",
    "```\n",
    "\n",
    "Now, your task is to find which of these 4 documents is most similar to the query `Who is responsible for a coding project and fixing others' mistakes?` using cosine similarity. You can reuse the `documents` and `normalized_embeddings_manual` arrays in your answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1bf0719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, embed the query:\n",
    "query_embedding = model.encode(\n",
    "    [\"Who is responsible for a coding project and fixing others' mistakes?\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "81fcd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second, normalize the query embedding:\n",
    "normalized_query_embedding = torch.nn.functional.normalize(\n",
    "    torch.from_numpy(query_embedding)\n",
    ").numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "885cdc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third, calculate the cosine similarity between the documents and the query by using the dot product:\n",
    "cosine_similarity_q3 = normalized_embeddings_manual @ normalized_query_embedding.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e1eeef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fourth, find the position of the vector with the highest cosine similarity:\n",
    "highest_cossim_position = cosine_similarity_q3.argmax()\n",
    "highest_cossim_position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb7f86a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bugs introduced by the intern had to be squashed by the lead developer.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fifth, find the document in that position in the `documents` array:\n",
    "documents[highest_cossim_position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0263ecda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
